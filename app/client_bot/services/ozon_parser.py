"""
–ü–∞—Ä—Å–µ—Ä –º–∞–≥–∞–∑–∏–Ω–æ–≤ Ozon
"""
import re
import logging
from typing import Optional, Dict, Any, List
from dataclasses import dataclass

import httpx
from bs4 import BeautifulSoup

from app.config import settings

logger = logging.getLogger(__name__)


class OzonParseError(Exception):
    """–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Ozon"""
    pass


@dataclass
class SellerData:
    """–î–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥–∞–≤—Ü–µ"""
    seller_id: str
    name: str
    rating: Optional[float] = None
    products_count: Optional[int] = None
    reviews_info: Optional[str] = None


@dataclass
class ProductComparison:
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ–≤–∞—Ä–∞ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏"""
    name: str
    seller_price: float
    best_price: float
    difference_percent: float
    recommendation: str


def extract_seller_id(url: str) -> Optional[str]:
    """
    –ò–∑–≤–ª–µ—á—å ID –ø—Ä–æ–¥–∞–≤—Ü–∞ –∏–∑ —Å—Å—ã–ª–∫–∏ Ozon

    Args:
        url: –°—Å—ã–ª–∫–∞ –Ω–∞ –º–∞–≥–∞–∑–∏–Ω

    Returns:
        ID –ø—Ä–æ–¥–∞–≤—Ü–∞ –∏–ª–∏ None
    """
    patterns = [
        r'ozon\.ru/seller/([a-zA-Z0-9_-]+)',
        r'ozon\.ru/brand/([a-zA-Z0-9_-]+)',
    ]

    for pattern in patterns:
        match = re.search(pattern, url, re.IGNORECASE)
        if match:
            seller_id = match.group(1).rstrip('/')
            return seller_id

    return None


async def parse_ozon_seller(seller_id: str) -> Dict[str, Any]:
    """
    –ü–∞—Ä—Å–∏–Ω–≥ –¥–∞–Ω–Ω—ã—Ö –æ –ø—Ä–æ–¥–∞–≤—Ü–µ Ozon —á–µ—Ä–µ–∑ ZenRows Scraper API

    Args:
        seller_id: ID –ø—Ä–æ–¥–∞–≤—Ü–∞

    Returns:
        –î–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥–∞–≤—Ü–µ

    Raises:
        OzonParseError: –ø—Ä–∏ –æ—à–∏–±–∫–µ –ø–∞—Ä—Å–∏–Ω–≥–∞
    """
    target_url = f"https://www.ozon.ru/seller/{seller_id}/"

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º ZenRows Scraper API –¥–ª—è –æ–±—Ö–æ–¥–∞ –∑–∞—â–∏—Ç—ã
    zenrows_key = settings.zenrows_api_key
    if not zenrows_key:
        raise OzonParseError("ZenRows API key –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω")

    # ZenRows API —Å JS-—Ä–µ–Ω–¥–µ—Ä–∏–Ω–≥–æ–º
    api_url = "https://api.zenrows.com/v1/"
    params = {
        "apikey": zenrows_key,
        "url": target_url,
        "js_render": "true",  # –†–µ–Ω–¥–µ—Ä–∏—Ç—å JavaScript
        "premium_proxy": "true",  # –ü—Ä–µ–º–∏—É–º –ø—Ä–æ–∫—Å–∏
    }

    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.get(api_url, params=params)

            if response.status_code != 200:
                logger.error(f"ZenRows API error: {response.status_code} - {response.text[:200]}")
                raise OzonParseError(f"HTTP {response.status_code}")

            soup = BeautifulSoup(response.text, 'lxml')

            result = {
                "seller_id": seller_id,
                "url": target_url,
                "name": _extract_seller_name(soup),
                "rating": _extract_seller_rating(soup),
                "products_count": _extract_products_count(soup),
                "products": [],
            }

            return result

    except httpx.TimeoutException:
        raise OzonParseError("Timeout –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ ZenRows")
    except httpx.RequestError as e:
        raise OzonParseError(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞: {e}")
    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ Ozon: {e}")
        raise OzonParseError(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞: {e}")


def _extract_seller_name(soup: BeautifulSoup) -> str:
    """–ò–∑–≤–ª–µ—á—å –Ω–∞–∑–≤–∞–Ω–∏–µ –ø—Ä–æ–¥–∞–≤—Ü–∞"""
    selectors = [
        'h1',
        '[data-widget="webSeller"] h1',
        '.seller-info__title',
    ]

    for selector in selectors:
        element = soup.select_one(selector)
        if element:
            return element.get_text(strip=True)

    return "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–¥–∞–≤–µ—Ü"


def _extract_seller_rating(soup: BeautifulSoup) -> Optional[float]:
    """–ò–∑–≤–ª–µ—á—å —Ä–µ–π—Ç–∏–Ω–≥ –ø—Ä–æ–¥–∞–≤—Ü–∞"""
    rating_pattern = r'(\d[.,]\d)\s*(?:–∏–∑\s*5|‚òÖ|–∑–≤—ë–∑–¥)'

    text = soup.get_text()
    match = re.search(rating_pattern, text)

    if match:
        try:
            return float(match.group(1).replace(',', '.'))
        except ValueError:
            pass

    return None


def _extract_products_count(soup: BeautifulSoup) -> Optional[int]:
    """–ò–∑–≤–ª–µ—á—å –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Ç–æ–≤–∞—Ä–æ–≤"""
    count_pattern = r'(\d+)\s*—Ç–æ–≤–∞—Ä'

    text = soup.get_text()
    match = re.search(count_pattern, text)

    if match:
        try:
            return int(match.group(1))
        except ValueError:
            pass

    return None


def format_audit_result(seller_data: Dict[str, Any]) -> str:
    """
    –§–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∞—É–¥–∏—Ç–∞ –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é

    Args:
        seller_data: –î–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–¥–∞–≤—Ü–µ

    Returns:
        –û—Ç—Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
    """
    name = seller_data.get("name", "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π –ø—Ä–æ–¥–∞–≤–µ—Ü")
    rating = seller_data.get("rating")
    products_count = seller_data.get("products_count")

    lines = [
        "üìä –ú–∏–Ω–∏-–∞—É–¥–∏—Ç –º–∞–≥–∞–∑–∏–Ω–∞ –Ω–∞ Ozon",
        "",
        f"–ú–∞–≥–∞–∑–∏–Ω: \"{name}\"",
    ]

    if rating:
        lines.append(f"‚≠ê –†–µ–π—Ç–∏–Ω–≥ –ø—Ä–æ–¥–∞–≤—Ü–∞: {rating}")

    if products_count:
        lines.append(f"üì¶ –¢–æ–≤–∞—Ä–æ–≤: {products_count} SKU")

    lines.extend([
        "",
        "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ",
        "",
        "üí° –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏:",
        "‚Ä¢ –ù–∞—Å—Ç—Ä–æ–π—Ç–µ –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ —Ü–µ–Ω –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤",
        "‚Ä¢ –°–ª–µ–¥–∏—Ç–µ –∑–∞ —Ä–µ–π—Ç–∏–Ω–≥–æ–º –∏ –æ—Ç–∑—ã–≤–∞–º–∏ –µ–∂–µ–¥–Ω–µ–≤–Ω–æ",
        "‚Ä¢ –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä—É–π—Ç–µ –æ—Ç—á—ë—Ç–Ω–æ—Å—Ç—å –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –≤—Ä–µ–º–µ–Ω–∏",
        "",
        "–•–æ—Ç–∏—Ç–µ –Ω–∞—Å—Ç—Ä–æ–∏—Ç—å –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏–π –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥?",
    ])

    return "\n".join(lines)
